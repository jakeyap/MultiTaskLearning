===================== EXP NOTES =====================

--------------- Experiments Completed ---------------
ModelA0 (length 2 classes, stance 5 classes)
exp1: SGD,epochs=20,lr=0.0005, moment=0.125, GPU=6, minibatch=24. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. Only have logs(6h) 
exp2: SGD,epochs=20,lr=0.0005, moment=0.250, GPU=6, minibatch=24. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. Only have logs(6h) 
exp3: SGD,epochs=20,lr=0.0005, moment=0.500, GPU=6, minibatch=24. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. Only have logs(6h) 
exp4: Adam,epochs=10,lr=0.00005, GPU=5, minibatch=25. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (2h30m) FAIL
exp5: Adam,epochs=10,lr=0.00010, GPU=5, minibatch=25. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (2h30m) MEH
exp6: Adam,epochs=10,lr=0.00020, GPU=5, minibatch=25. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (2h30m) FAIL
exp7: Adam,epochs=20,lr=0.00001, GPU=5, minibatch=25. weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (4h30m) OK
exp8: Adam,epochs=20,lr=0.00002, GPU=5, minibatch=25. weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (4h30m) MEH
exp9: Adam,epochs=20,lr=0.00005, GPU=5, minibatch=25. weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (4h30m) MEH
exp10: Adam,epochs=20,lr=0.00010, GPU=5, minibatch=25. weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (4h30m) FAIL
exp11: Adam,epochs=20,lr=0.00020, GPU=5, minibatch=25. weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (4h30m) FAIL

ModelBN (length 2 classes, stance 5 classes) N=hierarchy in transformers
exp12: Adam,epochs=10,lr=0.00001, GPU=5, minibatch=15, ModelB4, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (5h15m) OK
exp13: Adam,epochs=10,lr=0.00001, GPU=5, minibatch=15, ModelB3, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (5h) OK
exp14: Adam,epochs=10,lr=0.00001, GPU=5, minibatch=15, ModelB2, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (4h45m) OK
exp15: Adam,epochs=10,lr=0.00001, GPU=5, minibatch=15, ModelB1, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (4h30m) MEH

exp16: Adam,epochs=10,lr=0.00001, GPU=5, minibatch=15, ModelB4. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (4h15m) MEH
exp17: Adam,epochs=10,lr=0.00001, GPU=5, minibatch=15, ModelB3. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (4h) OK
exp18: Adam,epochs=10,lr=0.00001, GPU=5, minibatch=15, ModelB2. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (3h45m) MEH
exp19: Adam,epochs=10,lr=0.00001, GPU=5, minibatch=15, ModelB1. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (3h30m) MEH

exp20: Adam,epochs=10,lr=0.00001, GPU=2, minibatch=6, ModelC4, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (6h)
exp21: Adam,epochs=10,lr=0.00001, GPU=2, minibatch=6, ModelC3, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (5h45m)
exp22: Adam,epochs=10,lr=0.00001, GPU=2, minibatch=6, ModelC2, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (4h15m)
exp23: Adam,epochs=10,lr=0.00001, GPU=2, minibatch=6, ModelC1, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (4h)

ModelCN (length 2 classes, stance 5 classes) N=hierarchy in transformers. Emphasize the length loss with 3x learning rate
exp24: Adam,epochs=10,lr=0.00001, GPU=2, minibatch=4, ModelC4, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (?h)
exp25: Adam,epochs=10,lr=0.00001, GPU=2, minibatch=6, ModelC3, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (5h15m)
exp26: Adam,epochs=10,lr=0.00001, GPU=2, minibatch=6, ModelC2, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (5h15m)
exp27: Adam,epochs=10,lr=0.00001, GPU=2, minibatch=6, ModelC1, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (5h)

ModelBN (length 2 classes, stance 5 classes) N=hierarchy in transformers. Emphasize the length loss by double stepping length
exp28: Adam,epochs=10,lr=0.00001, GPU=2, minibatch=4, ModelC4, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (?h)
exp29: Adam,epochs=10,lr=0.00001, GPU=2, minibatch=6, ModelC3, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (?h)
exp30: Adam,epochs=10,lr=0.00001, GPU=2, minibatch=6, ModelC2, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=4, stance_class=5, length_class=2. (?h)

-------------- Experiments in progress --------------


---------------- Experiments planned ----------------
ModelDN (length 2 classes, stance 11 classes) N=hierarchy in transformers. On flattened coarse discourse
exp31: Adam,epochs=4,lr=0.00001, GPU=4, minibatch=16, ModelD4, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=8, EXPOSED=4, stance_class=11, length_class=2. (20h expected) 
exp32: Adam,epochs=4,lr=0.00001, GPU=4, minibatch=16, ModelD3, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=8, EXPOSED=4, stance_class=11, length_class=2. (20h expected)
exp33: Adam,epochs=4,lr=0.00001, GPU=4, minibatch=16, ModelD2, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=8, EXPOSED=4, stance_class=11, length_class=2. (20h expected)
exp34: Adam,epochs=4,lr=0.00001, GPU=4, minibatch=16, ModelD2, weighted stance loss. POST_PER_THREAD=256, POST_LENGTH=8, EXPOSED=4, stance_class=11, length_class=2. (20h expected)



===================== TASKS TODO ====================
build ModelDN
implement logic to do single task training on either task
try bert large (TOO large to fit into memory. discuss with teammates)
try to split into smaller buckets for length
implement single datapoint test 
eval - printing

===================== TASKS DONE ====================
install tweet tokenizer
try out tweet tokenizer
compare difference to bert tokenizer
merge coarse_discourse dataset with semeval17
tokenize only first 4 posts
stuck at the loss function. debug where it is going wrong
implement test function
implement check 1 test run
implement the length prediction loss
get the modified version to work
set up msi laptop as server at home
set up things on nus server
implement multi gpu
check f1 scoring bugs
to get the pkl files up on the remote pc, run the DataProcessor main function.
run 1 epoch of full training to time the model
implement model storing on best results
check accuracy measurement
modify hierarchical transformer model using huggingface
implement num workers on dataloading
log the steps in the previous work
write readme doc
try adam optimizer
try weighing the cost function to penalize the negative class
formatted SRQ dataset in the same way
tokenize SRQ dataset
implement a file to run tests on SRQ only, using older models
try weighing length cost function higher
