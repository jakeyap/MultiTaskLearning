===================== EXP NOTES =====================

--------------- Experiments Completed ---------------
exp1: SGD,epochs=20,lr=0.0005, moment=0.125, GPU=6, minibatch=24. Bug saving model/loss. Only have logs (6h) length acc=68%, stance f1=0.691
exp2: SGD,epochs=20,lr=0.0005, moment=0.250, GPU=6, minibatch=24. Bug saving model/loss. Only have logs (6h) length acc=67%, stance f1=0.666
exp3: SGD,epochs=20,lr=0.0005, moment=0.500, GPU=6, minibatch=24. Bug saving model/loss. Only have logs (6h) length acc=67%, stance f1=0.706
exp4: Adam,epochs=10,lr=0.00005, GPU=5, minibatch=25. (5h) 
exp5: Adam,epochs=10,lr=0.00010, GPU=5, minibatch=25. (5h) 
exp6: Adam,epochs=10,lr=0.00020, GPU=5, minibatch=25. (5h) FAIL
exp7: Adam,epochs=20,lr=0.00001, GPU=5, minibatch=25. weighted stance loss(?h) TODO: update results
exp8: Adam,epochs=20,lr=0.00002, GPU=5, minibatch=25. weighted stance loss(?h) TODO: update results
exp9: Adam,epochs=20,lr=0.00005, GPU=5, minibatch=25. weighted stance loss(?h) FAIL
exp10: Adam,epochs=20,lr=0.00010, GPU=5, minibatch=25. weighted stance loss(?h) FAIL
exp11: Adam,epochs=20,lr=0.00020, GPU=5, minibatch=25. weighted stance loss(?h) FAIL
-------------- Experiments in progress --------------
exp12: Adam,epochs=10,lr=0.00001, GPU=5, minibatch=15, ModelB4, weighted stance loss
exp13: Adam,epochs=10,lr=0.00001, GPU=5, minibatch=15, ModelB3, weighted stance loss
exp14: Adam,epochs=10,lr=0.00001, GPU=5, minibatch=15, ModelB2, weighted stance loss
exp15: Adam,epochs=10,lr=0.00001, GPU=5, minibatch=15, ModelB1, weighted stance loss
---------------- Experiments planned ----------------
exp16: Adam,epochs=10,lr=0.00001, GPU=5, minibatch=15, ModelB4
exp17: Adam,epochs=10,lr=0.00001, GPU=5, minibatch=15, ModelB3
exp18: Adam,epochs=10,lr=0.00001, GPU=5, minibatch=15, ModelB2
exp19: Adam,epochs=10,lr=0.00001, GPU=5, minibatch=15, ModelB1

===================== TASKS TODO ====================
implement logic to do single task training on either task
try bert large
try without masking URLs and hashtags
hmm how to include the SRQ tweet-pair dataset?
implement single datapoint test 
eval - printing


===================== TASKS DONE ====================
install tweet tokenizer
try out tweet tokenizer
compare difference to bert tokenizer
merge coarse_discourse dataset with semeval17
tokenize only first 4 posts
stuck at the loss function. debug where it is going wrong
implement test function
implement check 1 test run
implement the length prediction loss
get the modified version to work
set up msi laptop as server at home
set up things on nus server
implement multi gpu
check f1 scoring bugs
to get the pkl files up on the remote pc, run the DataProcessor main function.
run 1 epoch of full training to time the model
implement model storing on best results
check accuracy measurement
modify hierarchical transformer model using huggingface
implement num workers on dataloading
log the steps in the previous work
write readme doc
try adam optimizer
try weighing the cost function to penalize the negative class
